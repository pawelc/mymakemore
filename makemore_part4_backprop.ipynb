{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pawelc/mymakemore/blob/main/makemore_part4_backprop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3D6SryE-Voo"
      },
      "source": [
        "## makemore: becoming a backprop ninja\n",
        "\n",
        "swole doge style"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EqeFz0cr-Vop"
      },
      "outputs": [],
      "source": [
        "# there no change change in the first several cells from last lecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "REUJDZIC-Voq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n4EwbFSF-Voq",
        "outputId": "a4bb517c-f6d7-4cf7-b8a8-d83c2e75f26b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32033\n",
            "15\n",
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n"
          ]
        }
      ],
      "source": [
        "# read in all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))\n",
        "print(words[:8])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nDbiDosI-Vor",
        "outputId": "2f4691c5-dd9d-479b-f786-187da8de820f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ],
      "source": [
        "# build the vocabulary of characters and mappings to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Sup0S7vM-Vor",
        "outputId": "a286599b-1519-4794-aaf7-e62070ebf417",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ],
      "source": [
        "# build the dataset\n",
        "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
        "Xte,  Yte  = build_dataset(words[n2:])     # 10%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UCwwH9kw-Vor"
      },
      "outputs": [],
      "source": [
        "# ok biolerplate done, now we get to the action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "k7JNgdMK-Vos"
      },
      "outputs": [],
      "source": [
        "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
        "def cmp(s, dt, t):\n",
        "  ex = torch.all(dt == t.grad).item()\n",
        "  app = torch.allclose(dt, t.grad)\n",
        "  maxdiff = (dt - t.grad).abs().max().item()\n",
        "  print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8RB-bSJJ-Vos",
        "outputId": "529f3494-9fdc-4b56-db52-21f1780df520",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4137\n"
          ]
        }
      ],
      "source": [
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "# Note: I am initializating many of these parameters in non-standard ways\n",
        "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
        "# implementation of the backward pass.\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GSa8BzNh-Vos"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "n = batch_size # a shorter variable also, for convenience\n",
        "# construct a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WuNrAZ6B-Vos",
        "outputId": "03487de0-51e8-4353-d214-c6833e2146a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.3306, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
        "\n",
        "emb = C[Xb] # embed the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "# Linear layer 1\n",
        "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff**2\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "  p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "         embcat, emb]:\n",
        "  t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cqAKR5qu-Vot",
        "outputId": "0047f88e-26e8-4250-f92e-458048cb2f88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "bnbias          | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "bnraw           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bnvar_inv       | exact: False | approximate: True  | maxdiff: 2.7939677238464355e-09\n",
            "bnvar           | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bndiff2         | exact: False | approximate: True  | maxdiff: 2.9103830456733704e-11\n",
            "bndiff          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "bnmeani         | exact: False | approximate: True  | maxdiff: 3.725290298461914e-09\n",
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n",
            "embcat          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "W1              | exact: False | approximate: True  | maxdiff: 6.51925802230835e-09\n",
            "b1              | exact: False | approximate: True  | maxdiff: 3.6088749766349792e-09\n",
            "emb             | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "C               | exact: False | approximate: True  | maxdiff: 6.51925802230835e-09\n"
          ]
        }
      ],
      "source": [
        "# Exercise 1: backprop through the whole thing manually,\n",
        "# backpropagating through exactly all of the variables\n",
        "# as they are defined in the forward pass above, one by one\n",
        "\n",
        "dlogprobs = torch.zeros_like(logprobs)\n",
        "dlogprobs[range(n), Yb] = -1.0/n\n",
        "dprobs = (1.0 / probs) * dlogprobs\n",
        "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
        "dcounts = counts_sum_inv * dprobs\n",
        "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
        "dcounts += torch.ones_like(counts) * dcounts_sum\n",
        "dnorm_logits = counts * dcounts\n",
        "dlogits = dnorm_logits.clone()\n",
        "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
        "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
        "dh = dlogits @ W2.T\n",
        "dW2 = h.T @ dlogits\n",
        "db2 = dlogits.sum(0)\n",
        "dhpreact = (1.0 - h**2) * dh\n",
        "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "dbnraw = bngain * dhpreact\n",
        "dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "dbndiff = bnvar_inv * dbnraw\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "dbndiff += (2*bndiff) * dbndiff2\n",
        "dhprebn = dbndiff.clone()\n",
        "dbnmeani = (-dbndiff).sum(0)\n",
        "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "dembcat = dhprebn @ W1.T\n",
        "dW1 = embcat.T @ dhprebn\n",
        "db1 = dhprebn.sum(0)\n",
        "demb = dembcat.view(emb.shape)\n",
        "dC = torch.zeros_like(C)\n",
        "for k in range(Xb.shape[0]):\n",
        "  for j in range(Xb.shape[1]):\n",
        "    ix = Xb[k,j]\n",
        "    dC[ix] += demb[k,j]\n",
        "\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('probs', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('counts', dcounts, counts)\n",
        "cmp('norm_logits', dnorm_logits, norm_logits)\n",
        "cmp('logit_maxes', dlogit_maxes, logit_maxes)\n",
        "cmp('logits', dlogits, logits)\n",
        "cmp('h', dh, h)\n",
        "cmp('W2', dW2, W2)\n",
        "cmp('b2', db2, b2)\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "cmp('embcat', dembcat, embcat)\n",
        "cmp('W1', dW1, W1)\n",
        "cmp('b1', db1, b1)\n",
        "cmp('emb', demb, emb)\n",
        "cmp('C', dC, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mtHJxVE2-Vot",
        "outputId": "96170551-e584-4b09-8c69-23e35fc0bf5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.330595016479492 diff: 0.0\n"
          ]
        }
      ],
      "source": [
        "# Exercise 2: backprop through cross_entropy but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the loss,\n",
        "# take the derivative, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdims=True)\n",
        "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now:\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast - loss).item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wFudCkQm-Vou",
        "outputId": "1137c5d4-98ae-4f55-c5af-917655043cc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 5.3551048040390015e-09\n"
          ]
        }
      ],
      "source": [
        "# backward pass\n",
        "\n",
        "dlogits = F.softmax(logits, 1)\n",
        "dlogits[range(n), Yb] -= 1\n",
        "dlogits /= n\n",
        "\n",
        "cmp('logits', dlogits, logits) # I can only get approximate to be true, my maxdiff is 6e-9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zST9A-fO-Vou",
        "outputId": "8ae4a8e9-a63e-440e-cfc1-f2c433a8c91e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 27]), torch.Size([32]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "logits.shape, Yb.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XkhIgMdr-Vou",
        "outputId": "ded2134e-17e8-4470-9d8c-72d3370f445b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0662, 0.0885, 0.0190, 0.0519, 0.0181, 0.0824, 0.0247, 0.0352, 0.0178,\n",
              "        0.0311, 0.0372, 0.0366, 0.0391, 0.0289, 0.0352, 0.0139, 0.0081, 0.0184,\n",
              "        0.0154, 0.0554, 0.0526, 0.0216, 0.0258, 0.0707, 0.0577, 0.0264, 0.0221],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "F.softmax(logits, 1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "5pCKEVS6-Vou",
        "outputId": "9a6b562d-372c-4178-ac03-f27184da4fc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0662,  0.0885,  0.0190,  0.0519,  0.0181,  0.0824,  0.0247,  0.0352,\n",
              "        -0.9822,  0.0311,  0.0372,  0.0366,  0.0391,  0.0289,  0.0352,  0.0139,\n",
              "         0.0081,  0.0184,  0.0154,  0.0554,  0.0526,  0.0216,  0.0258,  0.0707,\n",
              "         0.0577,  0.0264,  0.0221], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "dlogits[0] * n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UMt-h0iW-Vou",
        "outputId": "163ac953-11f1-4a99-9f7b-e74aee12b102",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-2.3283e-10, grad_fn=<SumBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "dlogits[0].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Tp-8PpaJ-Vou",
        "outputId": "61972f8b-6f63-4bfb-ad20-f82ed600ae70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7d1445110c90>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(dlogits.detach(), cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "CLHpNfAe-Vov",
        "outputId": "bf6266e9-dca3-42c0-9c2e-f8512fb73433",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ],
      "source": [
        "# Exercise 3: backprop through batchnorm but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the output of batchnorm,\n",
        "# take the derivative w.r.t. its input, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "# bndiff2 = bndiff**2\n",
        "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
        "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "# hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# now:\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Sl1HB-pu-Vov",
        "outputId": "6301f09a-c04e-4a1e-d219-48ae92379184",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ],
      "source": [
        "# backward pass\n",
        "\n",
        "# before we had:\n",
        "# dbnraw = bngain * dhpreact\n",
        "# dbndiff = bnvar_inv * dbnraw\n",
        "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "# dbnvar = (-0.5*(bnvar + 1e-5)**-1.5) * dbnvar_inv\n",
        "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2) * dbnvar\n",
        "# dbndiff += (2*bndiff) * dbndiff2\n",
        "# dhprebn = dbndiff.clone()\n",
        "# dbnmeani = (-dbndiff).sum(0)\n",
        "# dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "\n",
        "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
        "# (you'll also need to use some of the variables from the forward pass up above)\n",
        "\n",
        "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn) # I can only get approximate to be true, my maxdiff is 9e-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kVEUwWlR-Vov",
        "outputId": "341dce80-de06-4e2e-df07-a45aa3cc5893",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 64]),\n",
              " torch.Size([1, 64]),\n",
              " torch.Size([1, 64]),\n",
              " torch.Size([32, 64]),\n",
              " torch.Size([64]))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "dhprebn.shape, bngain.shape, bnvar_inv.shape, dbnraw.shape, dbnraw.sum(0).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lS_sFLco-Vov",
        "outputId": "d015dab3-5cba-45ae-c952-47d852e4eb8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12297\n",
            "      0/ 200000: 3.7886\n",
            "  10000/ 200000: 2.2043\n",
            "  20000/ 200000: 2.3986\n",
            "  30000/ 200000: 2.4938\n",
            "  40000/ 200000: 1.9666\n",
            "  50000/ 200000: 2.3363\n",
            "  60000/ 200000: 2.3187\n",
            "  70000/ 200000: 2.0269\n",
            "  80000/ 200000: 2.3645\n",
            "  90000/ 200000: 2.1655\n",
            " 100000/ 200000: 1.9032\n",
            " 110000/ 200000: 2.3517\n",
            " 120000/ 200000: 2.0492\n",
            " 130000/ 200000: 2.4990\n",
            " 140000/ 200000: 2.3345\n",
            " 150000/ 200000: 2.1546\n",
            " 160000/ 200000: 1.9469\n",
            " 170000/ 200000: 1.8417\n",
            " 180000/ 200000: 2.0333\n",
            " 190000/ 200000: 1.8854\n"
          ]
        }
      ],
      "source": [
        "# Exercise 4: putting it all together!\n",
        "# Train the MLP neural net with your own backward pass\n",
        "\n",
        "# init\n",
        "n_embd = 10 # the dimensionality of the character embedding vectors\n",
        "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
        "\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
        "# Layer 1\n",
        "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
        "b1 = torch.randn(n_hidden,                        generator=g) * 0.1\n",
        "# Layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
        "# BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "  p.requires_grad = True\n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written (TODO)\n",
        "with torch.no_grad():\n",
        "\n",
        "  # kick off optimization\n",
        "  for i in range(max_steps):\n",
        "\n",
        "    # minibatch construct\n",
        "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "    Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
        "\n",
        "    # forward pass\n",
        "    emb = C[Xb] # embed the characters into vectors\n",
        "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
        "    # Linear layer\n",
        "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
        "    # BatchNorm layer\n",
        "    # -------------------------------------------------------------\n",
        "    bnmean = hprebn.mean(0, keepdim=True)\n",
        "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
        "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
        "    hpreact = bngain * bnraw + bnbias\n",
        "    # -------------------------------------------------------------\n",
        "    # Non-linearity\n",
        "    h = torch.tanh(hpreact) # hidden layer\n",
        "    logits = h @ W2 + b2 # output layer\n",
        "    loss = F.cross_entropy(logits, Yb) # loss function\n",
        "\n",
        "    # backward pass\n",
        "    for p in parameters:\n",
        "      p.grad = None\n",
        "    #loss.backward() # use this for correctness comparisons, delete it later!\n",
        "\n",
        "    # manual backprop! #swole_doge_meme\n",
        "    # -----------------\n",
        "    dlogits = F.softmax(logits, 1)\n",
        "    dlogits[range(n), Yb] -= 1\n",
        "    dlogits /= n\n",
        "    # 2nd layer backprop\n",
        "    dh = dlogits @ W2.T\n",
        "    dW2 = h.T @ dlogits\n",
        "    db2 = dlogits.sum(0)\n",
        "    # tanh\n",
        "    dhpreact = (1.0 - h**2) * dh\n",
        "    # batchnorm backprop\n",
        "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
        "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "    dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
        "    # 1st layer\n",
        "    dembcat = dhprebn @ W1.T\n",
        "    dW1 = embcat.T @ dhprebn\n",
        "    db1 = dhprebn.sum(0)\n",
        "    # embedding\n",
        "    demb = dembcat.view(emb.shape)\n",
        "    dC = torch.zeros_like(C)\n",
        "    for k in range(Xb.shape[0]):\n",
        "      for j in range(Xb.shape[1]):\n",
        "        ix = Xb[k,j]\n",
        "        dC[ix] += demb[k,j]\n",
        "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "    # -----------------\n",
        "\n",
        "    # update\n",
        "    lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "    for p, grad in zip(parameters, grads):\n",
        "      #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "      p.data += -lr * grad # new way of swole doge TODO: enable\n",
        "\n",
        "    # track stats\n",
        "    if i % 10000 == 0: # print every once in a while\n",
        "      print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "    lossi.append(loss.log10().item())\n",
        "\n",
        "  #   if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
        "  #     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "jxS93bMf-Vov"
      },
      "outputs": [],
      "source": [
        "# useful for checking your gradients\n",
        "# for p,g in zip(parameters, grads):\n",
        "#   cmp(str(tuple(p.shape)), g, p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZTcaFAzm-Vov"
      },
      "outputs": [],
      "source": [
        "# calibrate the batch norm at the end of training\n",
        "\n",
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Xu-6VE6Z-Vow",
        "outputId": "1fd2c5b3-7a73-40bf-8d5e-6b868f4a45ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 2.0731024742126465\n",
            "val 2.1125681400299072\n"
          ]
        }
      ],
      "source": [
        "# evaluate train and val loss\n",
        "\n",
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "AVTItzSn-Vow"
      },
      "outputs": [],
      "source": [
        "# I achieved:\n",
        "# train 2.0718822479248047\n",
        "# val 2.1162495613098145"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "j33df8Hd-Vow",
        "outputId": "fb04a02b-8860-4fa3-d6ed-20d9476173b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mora.\n",
            "mayah.\n",
            "seel.\n",
            "ndhayla.\n",
            "rethruthadrie.\n",
            "cailee.\n",
            "melin.\n",
            "shy.\n",
            "jenleigh.\n",
            "sana.\n",
            "arleitzion.\n",
            "kamin.\n",
            "shub.\n",
            "roshira.\n",
            "sten.\n",
            "joselle.\n",
            "jose.\n",
            "casubelleder.\n",
            "yarul.\n",
            "eli.\n"
          ]
        }
      ],
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # ------------\n",
        "      # forward pass:\n",
        "      # Embedding\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # ------------\n",
        "      # Sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      if ix == 0:\n",
        "        break\n",
        "\n",
        "    print(''.join(itos[i] for i in out))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "BeEY2G2E-Vow"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}